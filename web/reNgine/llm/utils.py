from django.contrib import messages
from dashboard.models import OllamaSettings
from reNgine.llm.config import LLM_CONFIG
import logging
from markdown import markdown

logger = logging.getLogger(__name__)

def get_default_llm_model():
    """
    Get the default LLM model from database or fallback to default
    Returns the model name as string
    """
    try:
        ollama_settings = OllamaSettings.objects.first()
        if ollama_settings and ollama_settings.selected_model:
            return ollama_settings.selected_model
    except Exception as e:
        logger.error(f"Error while retrieving default LLM model: {e}")
    
    # Fallback to default model from config based on provider
    try:
        if ollama_settings and ollama_settings.use_ollama:
            return LLM_CONFIG['providers']['ollama']['default_model']
        return LLM_CONFIG['providers']['openai']['default_model']
    except Exception as e:
        logger.error(f"Error while getting default model from config: {e}")
        return 'gpt-3.5-turbo'  # Ultimate fallback


def get_llm_vuln_input_description(title, path):
	vulnerability_description = ''
	vulnerability_description += f'Vulnerability Title: {title}'
	# llm gives concise vulnerability description when a vulnerable URL is provided
	vulnerability_description += f'\nVulnerable URL: {path}'

	return vulnerability_description

def convert_markdown_to_html(markdown_text):
    if markdown_text is None:
        return ""

    # Normalize non-string inputs to string
    if not isinstance(markdown_text, str):
        if isinstance(markdown_text, (list, tuple)):
            try:
                markdown_text = "\n".join(str(item) for item in markdown_text)
            except Exception:
                markdown_text = str(markdown_text)
        else:
            markdown_text = str(markdown_text)

    # Extract LLM badge if present (at the beginning of the text)
    llm_badge = ""
    if markdown_text.startswith('[LLM:'):
        llm_name = markdown_text[5:markdown_text.index(']')]
        llm_badge = f'<span class="badge bg-soft-primary text-primary mb-3">Generated by {llm_name}</span><br>'
        markdown_text = markdown_text[markdown_text.index(']')+1:].strip()
    
    # Configure Markdown with specific options
    html_content = markdown(markdown_text,
        extensions=[
            'fenced_code',
            'tables',
            'nl2br',
            'sane_lists',    # Better list handling
            'def_list',      # Definition lists support
        ],
    )

    # Add Bootstrap classes and clean up formatting
    html_content = (html_content
        .replace('<pre><code>', '<pre class="bg-light p-3 rounded"><code class="text-danger">')
        .replace('<ul>', '<ul class="list-unstyled">')
        .replace('<ol>', '<ul class="list-unstyled">')  # Convert ordered lists to unordered
        .replace('</ol>', '</ul>')
        .replace('\n\n', '<br>')
        .replace('\n', '')
    )
    
    return llm_badge + html_content

def is_empty_text(value) -> bool:
    """
    Check if a text value is empty or contains only whitespace/null-like values.
    
    Args:
        value: Any value to check for emptiness
        
    Returns:
        bool: True if the value is considered empty, False otherwise
    """
    try:
        if value is None:
            return True
        text = str(value).strip()
        if not text:
            return True
        # Treat list-like empties as empty as well
        normalized = text.replace('\n', '').replace('\r', '').replace(' ', '')
        if normalized in ('[]', '[\"\"]', '[\'\']', 'null', 'None'):
            return True
        return False
    except Exception:
        return True

def is_empty_attack_surface(value) -> bool:
    """
    Check if an attack surface value is empty, considering LLM tags.
    
    Args:
        value: The attack surface text to check
        
    Returns:
        bool: True if the value is considered empty, False otherwise
    """
    try:
        if not value:
            return True
        text = str(value)
        # Strip LLM tag if present
        if text.startswith('[LLM:') and ']' in text:
            text = text[text.index(']') + 1:]
        return len(text.strip()) == 0
    except Exception:
        return True

def is_empty_llm_report(model_obj) -> bool:
    """
    Check if an LLM vulnerability report is empty by checking all relevant fields.
    
    Args:
        model_obj: LLMVulnerabilityReport instance to check
        
    Returns:
        bool: True if all report fields are empty, False otherwise
    """
    fields = [
        getattr(model_obj, 'description', None),
        getattr(model_obj, 'impact', None),
        getattr(model_obj, 'remediation', None),
        getattr(model_obj, 'references', None),
    ]
    return all(is_empty_text(f) for f in fields)
