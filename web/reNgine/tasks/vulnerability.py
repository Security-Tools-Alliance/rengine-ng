import json
import os
import concurrent.futures
import shlex

from pathlib import Path

from reNgine.definitions import (
    ALL,
    CRLFUZZ,
    DALFOX,
    NUCLEI,
    NUCLEI_DEFAULT_TEMPLATES_PATH,
    S3SCANNER,
    VULNERABILITY_SCAN,
)
from reNgine.celery import app
from reNgine.celery_custom_task import RengineTask
from reNgine.tasks.notification import send_hackerone_report
from reNgine.tasks.command import run_command_line
from reNgine.utils.logger import default_logger as logger
from reNgine.utils.command_executor import stream_command
from reNgine.utils.command_builder import (
    CommandBuilder,
    build_piped_command,
    build_nuclei_cmd,
    build_dalfox_cmd,
    build_crlfuzz_cmd,
    build_s3scanner_cmd
)
from reNgine.utils.task_config import TaskConfig
from reNgine.utils.db import save_endpoint, save_vulnerability, subdomain_exists
from reNgine.utils.notifications import send_vulnerability_notification
from reNgine.utils.http import (
    get_subdomain_from_url,
    parse_curl_output,
    sanitize_url,
    prepare_urls_with_fallback,
)
from reNgine.utils.parsers import (
    parse_s3scanner_result,
    parse_nuclei_result,
    parse_dalfox_result,
    parse_crlfuzz_result,
)
from scanEngine.models import Hackerone
from startScan.models import (
    S3Bucket,
    ScanHistory,
    Subdomain,
    Vulnerability,
)

@app.task(name='vulnerability_scan', queue='io_queue', base=RengineTask, bind=True)
def vulnerability_scan(self, urls=None, ctx=None, description=None):
    """
        This function will serve as an entrypoint to vulnerability scan.
        All other vulnerability scan will be run from here including nuclei, crlfuzz, etc
    """
    from reNgine.utils.scan_helpers import execute_grouped_tasks

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}

    logger.info('üîí Running Vulnerability Scan Queue')
    config = TaskConfig(ctx, VULNERABILITY_SCAN)
    task_config = config.get_task_config()

    grouped_tasks = []
    if task_config['should_run_nuclei']:
        logger.info('üõ°Ô∏è  Nuclei Scan Preparation...')
        _task = nuclei_scan.si(
            urls=urls,
            ctx=ctx,
            description='Nuclei Scan'
        )
        logger.debug('üõ°Ô∏è  Nuclei Task Created')
        grouped_tasks.append(_task)

    if task_config['should_run_crlfuzz']:
        logger.info('üîç CRLFuzz Scan Preparation...')
        _task = crlfuzz_scan.si(
            urls=urls,
            ctx=ctx,
            description='CRLFuzz Scan'
        )
        logger.debug('üîç CRLFuzz Task Created')
        grouped_tasks.append(_task)

    if task_config['should_run_dalfox']:
        logger.info('üï∏Ô∏è  Dalfox XSS Scan Preparation...')
        _task = dalfox_scan.si(
            urls=urls,
            ctx=ctx,
            description='Dalfox XSS Scan'
        )
        logger.debug('üï∏Ô∏è  Dalfox Task Created')
        grouped_tasks.append(_task)

    if task_config['should_run_s3scanner']:
        logger.info('‚òÅÔ∏è  S3 Scanner Preparation...')
        _task = s3scanner.si(
            ctx=ctx,
            description='Misconfigured S3 Buckets Scanner'
        )
        logger.debug('‚òÅÔ∏è  S3 Scanner Task Created')
        grouped_tasks.append(_task)

    execute_grouped_tasks(
        self, 
        grouped_tasks, 
        task_name="vulnerability_scan", 
        callback_kwargs={'description': 'Processing vulnerability scan results'}
    )

    logger.info('üîí Vulnerability scan tasks submitted...')
    return {'status': 'submitted'}


@app.task(name='nuclei_scan', queue='io_queue', base=RengineTask, bind=True)
def nuclei_scan(self, urls=None, ctx=None, description=None):
    """HTTP vulnerability scan using Nuclei

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.

    Notes:
    Unfurl the urls to keep only domain and path, will be sent to vuln scan and
    ignore certain file extensions. Thanks: https://github.com/six2dez/reconftw
    """
    logger.info('üõ°Ô∏è  Starting Nuclei Scan Task')
    from reNgine.utils.scan_helpers import execute_grouped_tasks

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    # Initialize task config
    config = TaskConfig(ctx, VULNERABILITY_SCAN)
    task_config = config.get_task_config()
    
    # Get alive endpoints
    urls = prepare_urls_with_fallback(
        urls, 
        input_path=task_config['input_path'], 
        ctx=ctx,
        is_alive=True,
        ignore_files=True
    )

    if not urls:
        logger.warning('üîí No URLs to scan for Nuclei. Skipping.')
        return

    if task_config['intensity'] == 'normal': # reduce number of endpoints to scan
        if not os.path.exists(task_config['input_path']):
            with open(task_config['input_path'], 'w') as f:
                f.write('\n'.join(urls))

        unfurl_filter = config.get_working_dir(filename='urls_unfurled.txt')

        # Create the command components
        cat_cmd = CommandBuilder('cat').add_option(task_config['input_path'])
        unfurl_cmd = CommandBuilder('unfurl').add_option('format', '%s://%d%p')
        uro_cmd = CommandBuilder('poetry run uro')

        # Build the piped command
        cmd_builder_cat = build_piped_command([cat_cmd, unfurl_cmd, uro_cmd])
        cmd_builder_cat.add_redirection('>', unfurl_filter)
        
        run_command_line.delay(
            cmd_builder_cat.build_string(),
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)
        
        cmd_builder_sort = CommandBuilder('sort')
        cmd_builder_sort.add_option('-u')
        cmd_builder_sort.add_option(unfurl_filter)
        cmd_builder_sort.add_option('-o')
        cmd_builder_sort.add_option(unfurl_filter)
        
        run_command_line.delay(
            cmd_builder_sort.build_string(),
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)

        if not os.path.exists(unfurl_filter) or os.path.getsize(unfurl_filter) == 0:
            logger.error(f"üîí Failed to create or empty unfurled URLs file at {unfurl_filter}")
            unfurl_filter = task_config['input_path']

        task_config['input_path'] = unfurl_filter

    # Build templates
    logger.info('üîí Updating Nuclei templates ...')
    cmd = build_nuclei_cmd(config, update_templates=True)
    run_command_line.delay(
        cmd,
        shell=False,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )
    templates = []
    if not (task_config['nuclei_templates'] or task_config['custom_nuclei_templates']):
        templates.append(NUCLEI_DEFAULT_TEMPLATES_PATH)

    if task_config['nuclei_templates']:
        if ALL in task_config['nuclei_templates']:
            template = NUCLEI_DEFAULT_TEMPLATES_PATH
            templates.append(template)
        else:
            templates.extend(task_config['nuclei_templates'])

    if task_config['custom_nuclei_templates']:
        custom_nuclei_template_paths = [
            str(Path(NUCLEI_DEFAULT_TEMPLATES_PATH) / f'{str(elem)}.yaml') 
            for elem in task_config['custom_nuclei_templates']
        ]
        templates.extend(custom_nuclei_template_paths)

    # Build CMD base (without specific severity)
    cmd = build_nuclei_cmd(config, templates)

    # Execute tasks for each severity in parallel
    grouped_tasks = []
    custom_ctx = ctx
    for severity in task_config['severities']:
        custom_ctx['track'] = True
        logger.debug(f'üõ°Ô∏è  Creating Nuclei severity task for: {severity}')
        _task = nuclei_individual_severity_module.si(
            cmd,
            severity,
            task_config['enable_http_crawl'],
            task_config['should_fetch_gpt_report'],
            ctx=custom_ctx,
            description=f'Nuclei Scan with severity {severity}'
        )
        grouped_tasks.append(_task)
        logger.debug(f'üõ°Ô∏è  Added Nuclei {severity} severity task to group')

    logger.info('üõ°Ô∏è  Executing all Nuclei severity tasks as a group')
    execute_grouped_tasks(
        self,
        grouped_tasks,
        task_name="nuclei_scan",
        callback_kwargs={"scan_id": self.scan_id}
    )

    logger.info('üõ°Ô∏è  Vulnerability scan with all severities completed...')

    # GPT report handling
    process_vulnerability_gpt_reports(
        self, 
        NUCLEI, 
        'Getting Vulnerability GPT Report',
        task_config
    )

    return None


@app.task(name='nuclei_individual_severity_module', queue='cpu_queue', base=RengineTask, bind=True)
def nuclei_individual_severity_module(self, cmd, severity, enable_http_crawl, should_fetch_gpt_report, ctx=None, description=None):
    '''
        This celery task will run vulnerability scan in parallel.
        All severities supplied should run in parallel as grouped tasks.
    '''
    logger.info(f'üõ°Ô∏è  Starting Nuclei scan for severity: {severity}')
    from reNgine.utils.db import (
        record_exists,
        save_vulnerability,
        save_endpoint,
    )
    from reNgine.utils.notifications import send_vulnerability_scan_summary
    
    if ctx is None:
        ctx = {}

    results = []
    logger.info(f'Running vulnerability scan with severity: {severity}')
    
    if isinstance(cmd, list):
        cmd = ' '.join(cmd)

    cmd_parts = shlex.split(cmd)
    cmd_builder = CommandBuilder(cmd_parts[0])
    for i in range(1, len(cmd_parts)):
        cmd_builder.add_raw_option(cmd_parts[i])
    
    cmd_builder.add_option('-severity', severity)
    cmd_final = cmd_builder.build_list()
    
    for line in stream_command(
            cmd_final,
            shell=False,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id):

        if not isinstance(line, dict):
            continue

        results.append(line)

        # Gather nuclei results
        vuln_data = parse_nuclei_result(line)

        # Get corresponding subdomain
        http_url = sanitize_url(line.get('matched-at'))
        subdomain_name = get_subdomain_from_url(http_url)
        subdomain = subdomain_exists(subdomain_name, self.scan, self.domain)
        if not subdomain:
            continue

        # Look for duplicate vulnerabilities by excluding records that might change but are irrelevant.
        object_comparison_exclude = ['request','response', 'curl_command', 'tags', 'references', 'cve_ids', 'cwe_ids']

        # Add subdomain and target domain to the duplicate check
        vuln_data_copy = vuln_data.copy()
        vuln_data_copy['subdomain'] = subdomain
        vuln_data_copy['target_domain'] = self.domain

        # Check if record exists, if exists do not save it
        if record_exists(Vulnerability, data=vuln_data_copy, exclude_keys=object_comparison_exclude):
            logger.warning(f'Nuclei vulnerability of severity {severity} : {vuln_data_copy["name"]} for {subdomain_name} already exists')
            continue

        # Get or create EndPoint object
        response = line.get('response')
        httpx_crawl = False if response else enable_http_crawl # avoid yet another httpx crawl
        endpoint, _ = save_endpoint(
            http_url,
            crawl=httpx_crawl,
            subdomain=subdomain,
            ctx=ctx)
        if endpoint:
            http_url = endpoint.http_url
            if not httpx_crawl:
                output = parse_curl_output(response)
                endpoint.http_status = output['http_status']
                endpoint.save()

        # Get or create Vulnerability object
        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            subdomain=subdomain,
            **vuln_data)
        if not vuln:
            continue

        # Print vuln
        severity = line['info'].get('severity', 'unknown')
        logger.info(str(vuln))

        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name, 
            severity
        )

        # Send report to hackerone
        hackerone_query = Hackerone.objects.all()
        if (
            hackerone_query.exists()
            and severity not in ('info', 'low')
            and vuln.target_domain.h1_team_handle
        ):
            hackerone = hackerone_query.first()
            if (
                hackerone.send_critical
                and severity == 'critical'
                or hackerone.send_high
                and severity == 'high'
                or hackerone.send_medium
                and severity == 'medium'
            ):
                send_hackerone_report.delay(vuln.id)

    # Write results to JSON file
    with open(self.output_path, 'w') as f:
        json.dump(results, f, indent=4)

    send_vulnerability_scan_summary(self)

    return None

@app.task(name='dalfox_scan', queue='io_queue', base=RengineTask, bind=True)
def dalfox_scan(self, urls=None, ctx=None, description=None):
    logger.info('üï∏Ô∏è  Starting Dalfox XSS Scanner Task')
    from reNgine.utils.notifications import send_vulnerability_notification, send_vulnerability_scan_summary
    
    # Initialize task config
    config = TaskConfig(ctx, VULNERABILITY_SCAN)
    task_config = config.get_task_config()
    
    urls = prepare_urls_with_fallback(
        urls, 
        input_path=task_config['input_path'], 
        ctx=ctx,
        is_alive=True,
        ignore_files=False
    )

    if not urls:
        logger.warning('No URLs to scan for XSS. Skipping.')
        return

    # command builder
    cmd = build_dalfox_cmd(config)

    results = []
    for line in stream_command(
            cmd,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id,
            trunc_char=','
        ):
        if not isinstance(line, dict):
            continue

        results.append(line)

        vuln_data = parse_dalfox_result(line)

        http_url = sanitize_url(line.get('data'))
        subdomain_name = get_subdomain_from_url(http_url)

        subdomain = subdomain_exists(subdomain_name, self.scan, self.domain)
        if not subdomain:
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

        # Determination of the severity
        severity = {
            "1": "low",
            "2": "medium", 
            "3": "high",
            "4": "critical"
        }.get(str(vuln.severity), "medium")
        
        # Sending the notification
        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name,
            severity
        )

    # GPT report handling
    process_vulnerability_gpt_reports(
        self, 
        DALFOX, 
        'Getting Dalfox Vulnerability GPT Report',
        task_config
    )

    send_vulnerability_scan_summary(self)

    return results


@app.task(name='crlfuzz_scan', queue='io_queue', base=RengineTask, bind=True)
def crlfuzz_scan(self, urls=None, ctx=None, description=None):
    """CRLF Fuzzing with CRLFuzz

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    logger.info('üîç Starting CRLFuzz Scanner Task')
    from reNgine.utils.db import (
        save_vulnerability,
        save_endpoint,
    )
    from reNgine.utils.notifications import send_vulnerability_notification, send_vulnerability_scan_summary

    # Initialize task config
    config = TaskConfig(ctx, VULNERABILITY_SCAN)
    main_config = config.get_main_config()
    task_config = config.get_task_config()

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}

    urls = prepare_urls_with_fallback(
        urls, 
        input_path=task_config['crlfuzz_input_path'], 
        ctx=ctx,
        is_alive=True,
        ignore_files=False
    )

    if not urls:
        logger.warning('No URLs to scan for CRLF. Skipping.')
        return

    # command builder
    cmd = build_crlfuzz_cmd(config)

    run_command_line.delay(
        cmd,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )

    if not os.path.isfile(main_config['working_dir']):
        logger.info('No Results from CRLFuzz')
        return

    crlfs = []
    with open(config.get_working_dir(filename=self.filename), 'r') as file:
        crlfs = file.readlines()

    for crlf in crlfs:
        url = crlf.strip()

        vuln_data = parse_crlfuzz_result(url)

        http_url = sanitize_url(url)
        subdomain_name = get_subdomain_from_url(http_url)

        subdomain = subdomain_exists(subdomain_name, self.scan, self.domain)
        if not subdomain:
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

        # CRLF is generally considered medium
        severity = "medium"

        # Sending the notification
        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name,
            severity
        )

    # GPT report handling
    process_vulnerability_gpt_reports(
        self, 
        CRLFUZZ, 
        'Getting CRLFuzz Vulnerability GPT Report',
        task_config
    )

    send_vulnerability_scan_summary(self)

    return []

@app.task(name='s3scanner', queue='io_queue', base=RengineTask, bind=True)
def s3scanner(self, ctx=None, description=None):
    """Bucket Scanner

    Args:
        ctx (dict): Context
        description (str, optional): Task description shown in UI.
    """
    logger.info('‚òÅÔ∏è  Starting S3 Bucket Scanner Task')
    if ctx is None:
        ctx = {}

    config = TaskConfig(ctx, S3SCANNER)
    task_config = config.get_task_config()
    input_path = config.get_working_dir(filename=f'{self.scan_id}_s3_bucket_discovery.txt')

    subdomains = Subdomain.objects.filter(scan_history=self.scan)
    if not subdomains:
        logger.warning('No subdomains found for S3Scanner. Skipping.')
        return

    with open(input_path, 'w') as f:
        for subdomain in subdomains:
            f.write(subdomain.name + '\n')

    scan_history = ScanHistory.objects.filter(pk=self.scan_id).first()
    for provider in task_config['providers']:
        cmd = build_s3scanner_cmd(config, provider)

        for line in stream_command(
                cmd,
                shell=False,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id):

            if not isinstance(line, dict):
                continue

            if line.get('bucket', {}).get('exists', 0) == 1:
                result = parse_s3scanner_result(line)
                s3bucket, created = S3Bucket.objects.get_or_create(**result)
                scan_history.buckets.add(s3bucket)
                logger.info(f"s3 bucket added {result['provider']}-{result['name']}-{result['region']}")

def process_vulnerability_gpt_reports(self, source, log_message=None, task_config=None):
    """Process vulnerability GPT reports for a specific source
    
    Args:
        self: The task instance
        source (str): Source of vulnerabilities (NUCLEI, DALFOX, CRLFUZZ)
        log_message (str, optional): Custom log message
        task_config (dict, optional): Task configuration
        
    Returns:
        None
    """
    from reNgine.utils.db import get_vulnerability_gpt_report
    from dashboard.models import OpenAiAPIKey
    from startScan.models import Vulnerability
    from reNgine.settings import DEFAULT_THREADS

    # Use provided task_config or get it from self
    task_config = task_config or getattr(self, 'task_config', {})
    should_fetch_gpt_report = task_config.get('should_fetch_gpt_report', False)

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info(log_message or f'Getting {source} Vulnerability GPT Report')

        # Get vulnerabilities for specified source
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=source
        ).exclude(
            severity=0
        )

        # Process vulnerabilities differently based on source
        if source == 'nuclei':
            unique_vulns = {(vuln.name, vuln.get_path()) for vuln in vulns}
            vuln_data = list(unique_vulns)
        else:
            # For other sources, use name and URL
            vuln_data = [(vuln.name, vuln.http_url) for vuln in vulns]

        # Process vulnerabilities concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in vuln_data}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                vuln = future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.exception(f"Exception for Vulnerability {vuln}: {e}")
