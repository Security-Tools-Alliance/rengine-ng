import json
import os
import concurrent.futures
from pathlib import Path
from celery import group
from celery.utils.log import get_task_logger
from django.utils import html

from reNgine.celery import app
from reNgine.celery_custom_task import RengineTask
from reNgine.utilities.endpoint import get_http_urls, ensure_endpoints_crawled_and_execute
from reNgine.utilities.proxy import get_random_proxy
from reNgine.utilities.command import generate_header_param
from reNgine.utilities.data import is_iterable
from reNgine.utilities.database import save_endpoint, save_vulnerability
from reNgine.utilities.url import get_subdomain_from_url, sanitize_url
from reNgine.definitions import (
    BLIND_XSS_SERVER,
    CRLFUZZ,
    DALFOX,
    DEFAULT_SCAN_INTENSITY,
    DELAY,
    VULNERABILITY_SCAN,
    NUCLEI,
    RUN_NUCLEI,
    RUN_CRLFUZZ,
    RUN_DALFOX,
    RUN_S3SCANNER,
    FETCH_LLM_REPORT,
    CUSTOM_HEADER,
    INTENSITY,
    RATE_LIMIT,
    RETRIES,
    TIMEOUT,
    USER_AGENT,
    THREADS,
    NUCLEI_SEVERITY_MAP,
    DALFOX_SEVERITY_MAP,
    NUCLEI_TEMPLATES,
    NUCLEI_CUSTOM_TEMPLATES,
    NUCLEI_DEFAULT_TEMPLATES_PATH,
    S3SCANNER,
    PROVIDERS,
    S3SCANNER_DEFAULT_PROVIDERS,
    WAF_EVASION,
)
from reNgine.settings import (
    DEFAULT_THREADS,
    DEFAULT_HTTP_TIMEOUT,
    DEFAULT_RATE_LIMIT,
    DEFAULT_GET_LLM_REPORT,
    DEFAULT_RETRIES,
)
from scanEngine.models import Hackerone, Notification
from startScan.models import (
    Subdomain,
    Vulnerability,
    S3Bucket,
    ScanHistory,
)
from reNgine.tasks.command import stream_command, run_command
from reNgine.tasks.notification import send_hackerone_report
from reNgine.tasks.llm import llm_vulnerability_report
from dashboard.models import OpenAiAPIKey


logger = get_task_logger(__name__)


#-------------------------#
# Vulnerability Scanning #
#-------------------------#

@app.task(name='vulnerability_scan', queue='cpu_queue', bind=True, base=RengineTask)
def vulnerability_scan(self, urls=None, ctx=None, description=None):
    """
        This function will serve as an entrypoint to vulnerability scan.
        All other vulnerability scan will be run from here including nuclei, crlfuzz, etc
    """
    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    logger.info('Running Vulnerability Scan Queue')
    config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_run_nuclei = config.get(RUN_NUCLEI, True)
    should_run_crlfuzz = config.get(RUN_CRLFUZZ, False)
    should_run_dalfox = config.get(RUN_DALFOX, False)
    should_run_s3scanner = config.get(RUN_S3SCANNER, True)

    grouped_tasks = []
    if should_run_nuclei:
        _task = nuclei_scan.si(
            urls=urls,
            ctx=ctx,
            description='Nuclei Scan'
        )
        grouped_tasks.append(_task)

    if should_run_crlfuzz:
        _task = crlfuzz_scan.si(
            urls=urls,
            ctx=ctx,
            description='CRLFuzz Scan'
        )
        grouped_tasks.append(_task)

    if should_run_dalfox:
        _task = dalfox_xss_scan.si(
            urls=urls,
            ctx=ctx,
            description='Dalfox XSS Scan'
        )
        grouped_tasks.append(_task)

    if should_run_s3scanner:
        _task = s3scanner.si(
            ctx=ctx,
            description='Misconfigured S3 Buckets Scanner'
        )
        grouped_tasks.append(_task)

    # Launch tasks asynchronously without waiting for completion
    # This avoids Celery deadlock by not blocking the worker
    if grouped_tasks:
        celery_group = group(grouped_tasks)
        job = celery_group.apply_async()
        logger.info(f'Started {len(grouped_tasks)} vulnerability scan tasks asynchronously')
    else:
        logger.info('No vulnerability scan tasks to run')

    # return results
    return None


@app.task(name='nuclei_individual_severity_module', queue='io_queue', base=RengineTask, bind=True)
def nuclei_individual_severity_module(self, cmd, severity, should_fetch_llm_report, ctx=None, description=None):
    '''
        This celery task will run vulnerability scan in parallel.
        All severities supplied should run in parallel as grouped tasks.
    '''
    if ctx is None:
        ctx = {}
    results = []
    logger.info(f'Running vulnerability scan with severity: {severity}')
    cmd += f' -severity {severity}'
    # Send start notification
    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    for line in stream_command(
            cmd,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id):

        if not isinstance(line, dict):
            continue

        results.append(line)

        # Gather nuclei results
        vuln_data = parse_nuclei_result(line)

        # Get corresponding subdomain
        http_url = sanitize_url(line.get('matched-at'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping vulnerability scan for this subdomain.')
            continue

        # Look for duplicate vulnerabilities by excluding records that might change but are irrelevant.
        object_comparison_exclude = ['response', 'curl_command', 'tags', 'references', 'cve_ids', 'cwe_ids']

        # Add subdomain and target domain to the duplicate check
        vuln_data_copy = vuln_data.copy()
        vuln_data_copy['subdomain'] = subdomain
        vuln_data_copy['target_domain'] = self.domain

        # Check if record exists, if exists do not save it
        if record_exists(Vulnerability, data=vuln_data_copy, exclude_keys=object_comparison_exclude):
            logger.warning(f'Nuclei vulnerability of severity {severity} : {vuln_data_copy["name"]} for {subdomain_name} already exists')
            continue

        # Get or create EndPoint object
        response = line.get('response')
        endpoint, _ = save_endpoint(
            http_url=http_url,
            subdomain=subdomain,
            ctx=ctx)
        if endpoint:
            http_url = endpoint.http_url

        # Get or create Vulnerability object
        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            subdomain=subdomain,
            **vuln_data)
        if not vuln:
            continue

        # Print vuln
        severity = line['info'].get('severity', 'unknown')
        logger.warning(str(vuln))


        # Send notification for all vulnerabilities except info
        url = vuln.http_url or vuln.subdomain
        if send_vuln := (
            notif
            and notif.send_vuln_notif
            and vuln
            and severity in ['low', 'medium', 'high', 'critical']
        ):
            fields = {
                'Severity': f'**{severity.upper()}**',
                'URL': http_url,
                'Subdomain': subdomain_name,
                'Name': vuln.name,
                'Type': vuln.type,
                'Description': vuln.description,
                'Template': vuln.template_url,
                'Tags': vuln.get_tags_str(),
                'CVEs': vuln.get_cve_str(),
                'CWEs': vuln.get_cwe_str(),
                'References': vuln.get_refs_str()
            }
            severity_map = {
                'low': 'info',
                'medium': 'warning',
                'high': 'error',
                'critical': 'error'
            }
            self.notify(
                f'vulnerability_scan_#{vuln.id}',
                severity_map[severity],
                fields,
                add_meta_info=False)

        # Send report to hackerone
        hackerone_query = Hackerone.objects.all()
        if send_report := (
            hackerone_query.exists()
            and severity not in ('info', 'low')
            and vuln.target_domain.h1_team_handle
        ):
            hackerone = hackerone_query.first()
            if (
                (hackerone.send_critical and severity == 'critical')
                or (hackerone.send_high and severity == 'high')
                or (hackerone.send_medium and severity == 'medium')
            ):
                send_hackerone_report.delay(vuln.id)
    # Write results to JSON file
    with open(self.output_path, 'w') as f:
        json.dump(results, f, indent=4)

    # Send finish notif
    if send_status:
        vulns = Vulnerability.objects.filter(scan_history__id=self.scan_id)
        info_count = vulns.filter(severity=0).count()
        low_count = vulns.filter(severity=1).count()
        medium_count = vulns.filter(severity=2).count()
        high_count = vulns.filter(severity=3).count()
        critical_count = vulns.filter(severity=4).count()
        unknown_count = vulns.filter(severity=-1).count()
        vulnerability_count = info_count + low_count + medium_count + high_count + critical_count + unknown_count
        fields = {
            'Total': vulnerability_count,
            'Critical': critical_count,
            'High': high_count,
            'Medium': medium_count,
            'Low': low_count,
            'Info': info_count,
            'Unknown': unknown_count
        }
        self.notify(fields=fields)

    # after vulnerability scan is done, we need to run llm if
    # should_fetch_llm_report and openapi key exists

    if should_fetch_llm_report and OpenAiAPIKey.objects.exists():
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=NUCLEI
        ).exclude(
            severity=0
        )
        unique_vulns = {(vuln.name, vuln.get_path()) for vuln in vulns}
        unique_vulns = list(unique_vulns)

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_llm = {executor.submit(llm_vulnerability_report, vuln): vuln for vuln in unique_vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_llm):
                vuln = future_to_llm[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln[0]} - {vuln[1]}: {e}")  # Display title and path

        return None


@app.task(name='nuclei_scan', queue='io_queue', base=RengineTask, bind=True)
def nuclei_scan(self, urls=[], ctx={}, description=None):
    """Nuclei vulnerability scanner.

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    
    def _execute_nuclei_scan(ctx, description):
        # Initialize urls from outer scope
        nonlocal urls
        
        # Config
        config = self.yaml_configuration.get(VULNERABILITY_SCAN, {})
        nuclei_config = config.get(NUCLEI, {})
        should_fetch_llm_report = nuclei_config.get(FETCH_LLM_REPORT, DEFAULT_GET_LLM_REPORT)
        custom_header = nuclei_config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
        if custom_header:
            custom_header = generate_header_param(custom_header, 'common')
        intensity = nuclei_config.get(INTENSITY) or self.yaml_configuration.get(INTENSITY, DEFAULT_SCAN_INTENSITY)
        rate_limit = nuclei_config.get(RATE_LIMIT) or self.yaml_configuration.get(RATE_LIMIT, DEFAULT_RATE_LIMIT)
        retries = nuclei_config.get(RETRIES) or self.yaml_configuration.get(RETRIES, DEFAULT_RETRIES)
        timeout = nuclei_config.get(TIMEOUT) or self.yaml_configuration.get(TIMEOUT, DEFAULT_HTTP_TIMEOUT)
        user_agent = nuclei_config.get(USER_AGENT) or self.yaml_configuration.get(USER_AGENT)
        threads = nuclei_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
        nuclei_templates = nuclei_config.get(NUCLEI_TEMPLATES, [])
        custom_nuclei_templates = nuclei_config.get(NUCLEI_CUSTOM_TEMPLATES, [])
        input_path = str(Path(self.results_dir) / 'input_endpoints_nuclei.txt')
        proxy = get_random_proxy()

        # Get alive endpoints
        if not urls or not any(url for url in urls if url):
            logger.debug('Getting alive endpoints for Nuclei scan')
            urls = get_http_urls(
                is_alive=True,
                ignore_files=True,
                write_filepath=input_path,
                ctx=ctx
            )

        if not urls:
            logger.error('No alive URLs found for Nuclei scan. Skipping.')
            return

        if intensity == 'normal': # reduce number of endpoints to scan
            if not os.path.exists(input_path):
                with open(input_path, 'w') as f:
                    f.write('\n'.join(urls))

            unfurl_filter = str(Path(self.results_dir) / 'urls_unfurled.txt')
            
            run_command(
                f"cat {input_path} | unfurl -u format %s://%d%p |uro > {unfurl_filter}",
                shell=True,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id)
            run_command(
                f'sort -u {unfurl_filter} -o {unfurl_filter}',
                shell=True,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id)
                
            if not os.path.exists(unfurl_filter) or os.path.getsize(unfurl_filter) == 0:
                logger.error(f"Failed to create or empty unfurled URLs file at {unfurl_filter}")
                unfurl_filter = input_path
                
            input_path = unfurl_filter

        # Build templates
        logger.info('Updating Nuclei templates ...')
        run_command(
            'nuclei -update-templates',
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)
        templates = []
        if not (nuclei_templates or custom_nuclei_templates):
            templates.append(NUCLEI_DEFAULT_TEMPLATES_PATH)
        else:
            if nuclei_templates:
                templates.extend(nuclei_templates)
            if custom_nuclei_templates:
                templates.extend(custom_nuclei_templates)

        # Build cmd
        cmd = 'nuclei'
        cmd += f' -target {input_path}'
        cmd += f' -H {custom_header}' if custom_header else ''
        cmd += f' -t {",".join(templates)}' if templates else ''
        cmd += f' -rl {rate_limit}' if rate_limit and rate_limit > 0 else ''
        cmd += f' -retries {retries}' if retries and retries > 0 else ''
        cmd += f' -timeout {timeout}' if timeout and timeout > 0 else ''
        cmd += f' -c {threads}' if threads and threads > 0 else ''
        cmd += f' -proxy {proxy}' if proxy else ''
        cmd += f' -H "User-Agent: {user_agent}"' if user_agent else ''
        cmd += ' -jsonl'
        cmd += f' -o {self.output_path}'

        run_command(
            cmd,
            shell=False,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)

        results = []
        if not os.path.isfile(self.output_path):
            logger.error(f'Nuclei json output file {self.output_path} not found.')
            return results

        # Parse results
        with open(self.output_path, 'r') as f:
            for line in f:
                if line.strip():
                    try:
                        result = json.loads(line)
                        results.append(result)
                        parsed_result = parse_nuclei_result(result)
                        if parsed_result:
                            vuln = save_vulnerability(
                                target_domain=self.domain,
                                scan_history=self.scan,
                                subscan=self.subscan,
                                **parsed_result
                            )
                            if vuln and should_fetch_llm_report:
                                llm_vulnerability_report.delay(vuln.id)
                    except json.JSONDecodeError:
                        logger.error(f'Invalid JSON in nuclei output: {line}')
                        continue

        return results
    
    # Use the smart crawl-then-execute pattern
    return ensure_endpoints_crawled_and_execute(_execute_nuclei_scan, ctx, description)


@app.task(name='dalfox_xss_scan', queue='io_queue', base=RengineTask, bind=True)
def dalfox_xss_scan(self, urls=None, ctx=None, description=None):
    """XSS Scan using dalfox

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_fetch_llm_report = vuln_config.get(FETCH_LLM_REPORT, DEFAULT_GET_LLM_REPORT)
    dalfox_config = vuln_config.get(DALFOX) or {}
    custom_header = dalfox_config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
    if custom_header:
        custom_header = generate_header_param(custom_header, 'dalfox')
    proxy = get_random_proxy()
    is_waf_evasion = dalfox_config.get(WAF_EVASION, False)
    blind_xss_server = dalfox_config.get(BLIND_XSS_SERVER)
    user_agent = dalfox_config.get(USER_AGENT) or self.yaml_configuration.get(USER_AGENT)
    timeout = dalfox_config.get(TIMEOUT)
    delay = dalfox_config.get(DELAY)
    threads = dalfox_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    input_path = str(Path(self.results_dir) / 'input_endpoints_dalfox_xss.txt')

    if urls and is_iterable(urls) and any(url for url in urls if url):
        with open(input_path, 'w') as f:
            f.write('\n'.join(urls))
    else:
        urls = get_http_urls(
            is_alive=True,
            ignore_files=False,
            write_filepath=input_path,
            ctx=ctx
        )

    if not urls:
        logger.error('No URLs to scan for XSS. Skipping.')
        return

    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    # command builder
    cmd = 'dalfox --silence --no-color --no-spinner'
    cmd += ' --only-poc r '
    cmd += ' --ignore-return 302,404,403'
    cmd += ' --skip-bav'
    cmd += f' file {input_path}'
    cmd += f' --proxy {proxy}' if proxy else ''
    cmd += ' --waf-evasion' if is_waf_evasion else ''
    cmd += f' -b {blind_xss_server}' if blind_xss_server else ''
    cmd += f' --delay {delay}' if delay else ''
    cmd += f' --timeout {timeout}' if timeout else ''
    cmd += f' --user-agent {user_agent}' if user_agent else ''
    cmd += f' {custom_header}' if custom_header else ''
    cmd += f' --worker {threads}' if threads else ''
    cmd += ' --format json'

    results = []
    for line in stream_command(
            cmd,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id,
            trunc_char=','
        ):
        if not isinstance(line, dict):
            continue

        results.append(line)

        vuln_data = parse_dalfox_result(line)

        http_url = sanitize_url(line.get('data'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping dalfox scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url=http_url,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

    # after vulnerability scan is done, we need to run llm if
    # should_fetch_llm_report and openapi key exists

    if should_fetch_llm_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting Dalfox Vulnerability LLM Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=DALFOX
        ).exclude(
            severity=0
        )

        _vulns = []
        _vulns.extend((vuln.name, vuln.http_url) for vuln in vulns)
        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_llm = {executor.submit(llm_vulnerability_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_llm):
                vuln = future_to_llm[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln[0]} - {vuln[1]}: {e}")  # Display title and path
    return results


@app.task(name='crlfuzz_scan', queue='io_queue', base=RengineTask, bind=True)
def crlfuzz_scan(self, urls=None, ctx=None, description=None):
    """CRLF Fuzzing with CRLFuzz

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_fetch_llm_report = vuln_config.get(FETCH_LLM_REPORT, DEFAULT_GET_LLM_REPORT)
    custom_header = vuln_config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
    if custom_header:
        custom_header = generate_header_param(custom_header, 'common')
    proxy = get_random_proxy()
    user_agent = vuln_config.get(USER_AGENT) or self.yaml_configuration.get(USER_AGENT)
    threads = vuln_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    input_path = str(Path(self.results_dir) / 'input_endpoints_crlf.txt')
    output_path = str(Path(self.results_dir) / f'{self.filename}')

    if urls and is_iterable(urls) and any(url for url in urls if url):
        with open(input_path, 'w') as f:
            f.write('\n'.join(urls))
    else:
        urls = get_http_urls(
            is_alive=True,
            ignore_files=True,
            write_filepath=input_path,
            ctx=ctx
        )

    if not urls:
        logger.error('No URLs to scan for CRLF. Skipping.')
        return

    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    # command builder
    cmd = 'crlfuzz -s'
    cmd += f' -l {input_path}'
    cmd += f' -x {proxy}' if proxy else ''
    cmd += f' {custom_header}' if custom_header else ''
    cmd += f' -o {output_path}'

    run_command(
        cmd,
        shell=False,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )

    if not os.path.isfile(output_path):
        logger.info('No Results from CRLFuzz')
        return

    crlfs = []
    with open(output_path, 'r') as file:
        crlfs = file.readlines()

    for crlf in crlfs:
        url = crlf.strip()

        vuln_data = parse_crlfuzz_result(url)

        http_url = sanitize_url(url)
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping crlfuzz scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url=http_url,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

    # after vulnerability scan is done, we need to run llm if
    # should_fetch_llm_report and openapi key exists

    if should_fetch_llm_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting CRLFuzz Vulnerability LLM Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=CRLFUZZ
        ).exclude(
            severity=0
        )

        _vulns = []
        _vulns.extend((vuln.name, vuln.http_url) for vuln in vulns)
        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_llm = {executor.submit(llm_vulnerability_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_llm):
                vuln = future_to_llm[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln[0]} - {vuln[1]}: {e}")  # Display title and path

    return []


@app.task(name='s3scanner', queue='io_queue', base=RengineTask, bind=True)
def s3scanner(self, ctx=None, description=None):
    """Bucket Scanner

    Args:
        ctx (dict): Context
        description (str, optional): Task description shown in UI.
    """
    if ctx is None:
        ctx = {}
    input_path = str(Path(self.results_dir) / f'{self.scan_id}_s3_bucket_discovery.txt')

    subdomains = Subdomain.objects.filter(scan_history=self.scan)
    if not subdomains:
        logger.error('No subdomains found for S3Scanner. Skipping.')
        return

    with open(input_path, 'w') as f:
        for subdomain in subdomains:
            f.write(subdomain.name + '\n')

    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    s3_config = vuln_config.get(S3SCANNER) or {}
    threads = s3_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    providers = s3_config.get(PROVIDERS, S3SCANNER_DEFAULT_PROVIDERS)
    scan_history = ScanHistory.objects.filter(pk=self.scan_id).first()
    for provider in providers:
        cmd = f's3scanner -bucket-file {input_path} -enumerate -provider {provider} -threads {threads} -json'
        for line in stream_command(
                cmd,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id):

            if not isinstance(line, dict):
                continue

            if line.get('bucket', {}).get('exists', 0) == 1:
                result = parse_s3scanner_result(line)
                s3bucket, created = S3Bucket.objects.get_or_create(**result)
                scan_history.buckets.add(s3bucket)
                logger.info(f"s3 bucket added {result['provider']}-{result['name']}-{result['region']}")


#--------------------#
# Parsing Functions  #
#--------------------#

def parse_s3scanner_result(line):
    '''
        Parses and returns s3Scanner Data
    '''
    bucket = line['bucket']
    return {
        'name': bucket['name'],
        'region': bucket['region'],
        'provider': bucket['provider'],
        'owner_display_name': bucket['owner_display_name'],
        'owner_id': bucket['owner_id'],
        'perm_auth_users_read': bucket['perm_auth_users_read'],
        'perm_auth_users_write': bucket['perm_auth_users_write'],
        'perm_auth_users_read_acl': bucket['perm_auth_users_read_acl'],
        'perm_auth_users_write_acl': bucket['perm_auth_users_write_acl'],
        'perm_auth_users_full_control': bucket['perm_auth_users_full_control'],
        'perm_all_users_read': bucket['perm_all_users_read'],
        'perm_all_users_write': bucket['perm_all_users_write'],
        'perm_all_users_read_acl': bucket['perm_all_users_read_acl'],
        'perm_all_users_write_acl': bucket['perm_all_users_write_acl'],
        'perm_all_users_full_control': bucket['perm_all_users_full_control'],
        'num_objects': bucket['num_objects'],
        'size': bucket['bucket_size']
    }

def parse_nuclei_result(line):
    """Parse results from nuclei JSON output.

    Args:
        line (dict): Nuclei JSON line output.

    Returns:
        dict: Vulnerability data.
    """
    return {
        'name': line.get('info', {}).get('name', ''),
        'type': line.get('type', ''),
        'severity': NUCLEI_SEVERITY_MAP.get(
            line.get('info', {}).get('severity', 'unknown'), 0
        ),
        'template': line.get('template-path', '').replace(
            f'{NUCLEI_DEFAULT_TEMPLATES_PATH}/', ''
        ),
        'template_url': line.get('template-url', ''),
        'template_id': line.get('template-id', ''),
        'description': line.get('info', {}).get('description', ''),
        'matcher_name': line.get('matcher-name', ''),
        'curl_command': line.get('curl-command'),
        'request': html.escape(line.get('request', '')),
        'response': html.escape(line.get('response', '')),
        'extracted_results': line.get('extracted-results', []),
        'cvss_metrics': line.get('info', {})
        .get('classification', {})
        .get('cvss-metrics', ''),
        'cvss_score': line.get('info', {})
        .get('classification', {})
        .get('cvss-score'),
        'cve_ids': line.get('info', {})
        .get('classification', {})
        .get('cve_id', [])
        or [],
        'cwe_ids': line.get('info', {})
        .get('classification', {})
        .get('cwe_id', [])
        or [],
        'references': line.get('info', {}).get('reference', []) or [],
        'tags': line.get('info', {}).get('tags', []),
        'source': NUCLEI,
    }

def parse_dalfox_result(line):
    """Parse results from nuclei JSON output.

    Args:
        line (dict): Nuclei JSON line output.

    Returns:
        dict: Vulnerability data.
    """

    description = ''
    description += f" Evidence: {line.get('evidence')} <br>" if line.get('evidence') else ''
    description += f" Message: {line.get('message')} <br>" if line.get('message') else ''
    description += f" Payload: {line.get('message_str')} <br>" if line.get('message_str') else ''
    description += f" Vulnerable Parameter: {line.get('param')} <br>" if line.get('param') else ''

    return {
        'name': 'XSS (Cross Site Scripting)',
        'type': 'XSS',
        'severity': DALFOX_SEVERITY_MAP[line.get('severity', 'unknown')],
        'description': description,
        'source': DALFOX,
        'cwe_ids': [line.get('cwe')]
    }


def parse_crlfuzz_result(url):
    """Parse CRLF results

    Args:
        url (str): CRLF Vulnerable URL

    Returns:
        dict: Vulnerability data.
    """

    return {
        'name': 'CRLF (HTTP Response Splitting)',
        'type': 'CRLF',
        'severity': 2,
        'description': 'A CRLF (HTTP Response Splitting) vulnerability has been discovered.',
        'source': CRLFUZZ,
    }


def record_exists(model, data, exclude_keys=None):
    """
    Check if a record already exists in the database based on the given data.

    Args:
        model (django.db.models.Model): The Django model to check against.
        data (dict): Data dictionary containing fields and values.
        exclude_keys (list): Keys to exclude from the comparison.

    Returns:
        bool: True if record exists, False otherwise.
    """
    if exclude_keys is None:
        exclude_keys = []
    lookup_data = {k: v for k, v in data.items() if k not in exclude_keys}
    return model.objects.filter(**lookup_data).exists() 